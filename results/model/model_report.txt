Model Report
============
Credit Scoring Project - Model Training & Evaluation Report

Algorithm
---------
The model used is a Logistic Regression classifier.
We utilized `sklearn.linear_model.LogisticRegression` with the following key parameters:
- `class_weight='balanced'`: To handle the imbalance in the dataset (far fewer defaults than non-defaults).
- `max_iter=3000`: To ensure convergence.
- `solver='lbfgs'`: A standard optimizer for logistic regression.
- `StandardScaler`: Applied to features before the model to ensure all features are on the same scale, which is crucial for Logistic Regression.

Cross-Validation Results
------------------------
5-Fold Stratified Cross-Validation was performed:
- The model achieves consistent AUC scores across folds, indicating stable performance.
- Mean CV AUC: ~0.72-0.74 (varies based on feature engineering)
- Standard deviation across folds is low, showing robust generalization.

Learning Curve Analysis (Overfitting Prevention)
------------------------------------------------
See: results/model/learning_curve.png

The learning curve shows:
1. **Training AUC**: Starts high and remains relatively stable
2. **Validation AUC**: Converges toward training AUC as data increases

Key Observations:
- The gap between training and validation scores is small, indicating LOW OVERFITTING
- Both curves plateau, suggesting more data would have diminishing returns
- Validation score is close to training score = good generalization

Measures Taken to Prevent Overfitting:
1. **L2 Regularization**: Built into Logistic Regression (default penalty='l2')
2. **Class Weighting**: Using balanced weights prevents overfitting to majority class
3. **StandardScaler**: Normalizing features prevents coefficients from exploding
4. **Simple Model**: Logistic Regression is inherently less prone to overfitting than complex models

Stopping Criteria Justification:
- Logistic Regression with LBFGS converges based on gradient tolerance
- max_iter=3000 ensures full convergence without early stopping concerns
- Learning curve shows validation performance plateaus = no benefit from more complexity

Why Accuracy shouldn't be used?
-------------------------------
Accuracy is not a suitable metric for this problem because the classes are highly imbalanced.
The target variable (Default) has a much lower frequency than the non-default class (~8% default rate).
If a model simply predicted "No Default" for every customer, it would achieve a very high accuracy (~92%) but would be useless for identifying risk.

Instead, we use **ROC AUC (Receiver Operating Characteristic - Area Under Curve)**.
AUC measures the ability of the model to distinguish between classes:
- AUC = 0.5: Random guess (no discriminative power)
- AUC = 1.0: Perfect model
- Our target: AUC > 0.62 (achieved)

Other suitable metrics for imbalanced classification:
- Precision-Recall AUC
- F1-Score
- Gini Coefficient (2*AUC - 1)

Limits and Possible Improvements
--------------------------------
Current Limitations:
1. **Linearity Assumption**: Logistic Regression assumes a linear relationship between input features and the log-odds of the outcome. It may miss complex non-linear patterns.
2. **Feature Engineering**: The current model relies heavily on the quality of engineered features. While we included auxiliary data aggregations, more sophisticated features could help.
3. **Missing Interactions**: The model doesn't explicitly capture feature interactions (e.g., income Ã— loan amount).
4. **External Data**: Limited to provided data; real-world models often incorporate additional data sources.

Possible Improvements:
1. **Tree-based Models**: LightGBM, XGBoost, or CatBoost typically achieve AUC > 0.78 on this dataset with less preprocessing.
2. **Advanced Feature Engineering**:
   - Debt-to-income ratio: AMT_CREDIT / AMT_INCOME_TOTAL
   - Credit utilization trends from credit card data
   - Payment delay patterns from installments
   - Aggregations with different time windows
3. **Ensembling**: Combine predictions from multiple models (stacking, blending)
4. **Hyperparameter Tuning**: GridSearchCV or Bayesian Optimization for optimal regularization strength
5. **Feature Selection**: Use SHAP values or permutation importance to select most predictive features
6. **Target Encoding**: For high-cardinality categorical variables


ACTUAL TRAINING RESULTS
==============================
Cross-Validation AUCs: ['0.7471', '0.7583', '0.7520', '0.7563', '0.7468']
Mean CV AUC: 0.7521 (+/- 0.0047)
Number of features: 290
Training samples: 307511
